{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.14"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3714jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
      "display_name": "Python 3.7.14 64-bit"
    },
    "colab": {
      "provenance": []
    },
    "metadata": {
      "interpreter": {
        "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install spacy\n",
        "!pip install einops\n",
        "!pip install wandb\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "id": "fppJQNGMppou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh2o9nVyXdi3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import io\n",
        "from typing import List, Optional\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor, optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04DxhvcaXdi7"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH_U-Elmijlh"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "bDhOXRruJA4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Data preparation\n",
        "\n",
        "This section just follows https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html, for now."
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "cO9KjT-pXdi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')"
      ],
      "metadata": {
        "id": "xT_jZ-BxqgO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
      ],
      "metadata": {
        "id": "mpZFfyN3rroN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths"
      ],
      "metadata": {
        "id": "ldFlCG22r4GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')"
      ],
      "metadata": {
        "id": "Jha9rnpwr-DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "      counter.update(tokenizer(line))\n",
        "  vcab = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "  vcab.set_default_index(vcab['<unk>'])\n",
        "  return vcab\n",
        "\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
      ],
      "metadata": {
        "id": "BaEm35BBtYHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    de_tensor = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
        "    en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
        "    data.append((de_tensor, en_tensor))\n",
        "  return data"
      ],
      "metadata": {
        "id": "SZM9MvD-sH5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = preprocess(train_filepaths)\n",
        "val_data = preprocess(val_filepaths)\n",
        "test_data = preprocess(test_filepaths)"
      ],
      "metadata": {
        "id": "KBUTdELAuvTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "JcCGl-UQvj9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']"
      ],
      "metadata": {
        "id": "GuAUpQ460VXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])],dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])],dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "  return de_batch, en_batch"
      ],
      "metadata": {
        "id": "3JNOhe1w0mX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=gen_batch)\n",
        "val_dl = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=gen_batch)\n",
        "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=gen_batch)"
      ],
      "metadata": {
        "id": "lCh7iEGP06UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src, target = next(iter(train_dl))\n",
        "src.shape, target.shape"
      ],
      "metadata": {
        "id": "Nzt7AlSjRSaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer model\n",
        "\n",
        "With a lot of annotations to be cleaned up later"
      ],
      "metadata": {
        "id": "4NZEZ8RZxfYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_dim: int,\n",
        "               n_heads: int,\n",
        "               is_cross: bool):\n",
        "    super().__init__()\n",
        "    assert emb_dim % n_heads == 0, \"Embedding dimension must be divisble by number of heads\"\n",
        "    self.n_heads = n_heads\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_dim = emb_dim // n_heads\n",
        "    self.is_cross = is_cross\n",
        "    if not is_cross:\n",
        "      # This projects each word vector into a new vector space (and we have n_heads amount of different vector spaces)\n",
        "      self.kqv_project = nn.Linear(self.emb_dim, self.emb_dim * 3, bias=False) \n",
        "    else:\n",
        "      self.kv_project = nn.Linear(self.emb_dim, self.emb_dim * 2, bias=False)\n",
        "      self.q_project = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
        "    self.out_project = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "  \n",
        "  def forward(self, \n",
        "              query: Tensor, # [bsz, q_seq_len, emb_dim]\n",
        "              kv: Tensor, # [bsz, kv_seq_len, emb_dim]\n",
        "              # The mask is a tensor where masked positions are -inf and unmasked are 0s\n",
        "              # This is used for three cases currently: \n",
        "              # 1) attention mask for masking future tokens in decoder\n",
        "              # 2) padding mask for encoder \n",
        "              # 3) padding mask for decoder (where we want to mask out tokens that correspond to paddings in kv)\n",
        "              mask: Optional[Tensor] = None, # [bsz, 1, 1, kv_seq_len] or [bsz, 1, q_seq_len, q_seq_len] where q_seq_len == kv_seq_len for self-attention\n",
        "              ) -> Tensor:\n",
        "    bsz, kv_seq_len, _ = kv.shape\n",
        "    q_seq_len = query.shape[1]\n",
        "\n",
        "    if not self.is_cross: # self-attention\n",
        "      assert q_seq_len == kv_seq_len\n",
        "      kqv = self.kqv_project(kv) # [bsz, kv_seq_len, emb_dim * 3]\n",
        "      # reshape for multi-head attention\n",
        "      kqv = kqv.view(bsz, kv_seq_len, self.n_heads, self.head_dim * 3).transpose(1,2) # [bsz, n_heads, seq_len, head_dim * 3]\n",
        "      K, Q, V = kqv.chunk(3,dim=-1) # K=Q=V => [bsz, n_heads, seq_len, head_dim]\n",
        "    else:\n",
        "      # K, V should come from encoder values, and Q from decoder\n",
        "      kv_projected = self.kv_project(kv).view(bsz, kv_seq_len, self.n_heads, self.head_dim * 2).transpose(1,2)\n",
        "      K, V = kv_projected.chunk(2, dim=-1) # [bsz, n_heads, kv_seq_len, head_dim]\n",
        "      Q = self.q_project(query).view(bsz, -1, self.n_heads, self.head_dim).transpose(1,2) # [bsz, n_heads, q_seq_len, head_dim]\n",
        "\n",
        "    # print(f\"K shape: {K.shape}\\t V shape: {V.shape}\\t Q shape: {Q.shape}\")\n",
        "    attn_weights = torch.einsum('bhqd,bhkd->bhqk',[Q,K]) # [bsz, n_heads, q_seq_len, kv_seq_len] \n",
        "    attn_weights /= math.sqrt(self.head_dim) \n",
        "\n",
        "    if mask is not None:\n",
        "      attn_weights += mask\n",
        "\n",
        "    # softmax across last dim as it represents attention weight for each embedding vector in sequence\n",
        "    softmax_attn = F.softmax(attn_weights, dim=-1) \n",
        "    out = torch.einsum('bhql,bhld->bhqd',[softmax_attn, V]) # [bsz, n_heads, q_seq_len, head_dim]\n",
        "    out = out.transpose(1,2).reshape(bsz, -1, self.n_heads * self.head_dim) # [bsz, q_seq_len, emb_dim]\n",
        "    return self.out_project(out)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_dim: int,\n",
        "               hidden_dim: int,\n",
        "               n_heads: int):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadAttention(emb_dim, n_heads, is_cross=False)\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(emb_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(hidden_dim, emb_dim),\n",
        "    ) # feed forward network\n",
        "\n",
        "    # NOTE: why do we normalize across embedding dim only? \n",
        "    # see https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm\n",
        "    # and https://stackoverflow.com/questions/70065235/understanding-torch-nn-layernorm-in-nlp\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, \n",
        "              xs: Tensor, # [bsz, seq_len, emb_dim],\n",
        "              mask: Tensor, # (B,1,1,S)\n",
        "              ) -> Tensor:\n",
        "\n",
        "    ys = self.attn(query=xs, kv=xs, mask=mask) \n",
        "    imm = self.norm1(self.dropout(ys) + xs)\n",
        "\n",
        "    # [bsz,seq_len,emb_dim]\n",
        "    out = self.ff(imm) \n",
        "\n",
        "    return self.norm2(self.dropout(out) +imm) # same dim as input xs\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 emb_dim: int = 512, \n",
        "                 hidden_dim: int = 2048,\n",
        "                 n_heads: int = 8,\n",
        "                 depth: int = 8):\n",
        "        super().__init__()\n",
        "        encoder_block = TransformerEncoderBlock(emb_dim, hidden_dim, n_heads)\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_block) for i in range(depth)])\n",
        "\n",
        "    def forward(self, \n",
        "        xs: Tensor, # [bsz, seq_len, emb_dim] \n",
        "        mask: Tensor # (B,1,1,S)\n",
        "        ) -> Tensor:\n",
        "              \n",
        "        out = xs\n",
        "\n",
        "        for layer in self.layers:\n",
        "          out = layer(out, mask)\n",
        "\n",
        "        return out \n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_dim: int,\n",
        "               hidden_dim: int,\n",
        "               n_heads: int):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "    self.norm3 = nn.LayerNorm(emb_dim)\n",
        "    self.masked_attn = MultiHeadAttention(emb_dim, n_heads, False)\n",
        "    self.cross_attn = MultiHeadAttention(emb_dim, n_heads, True)\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(emb_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(hidden_dim, emb_dim),\n",
        "    ) # feed forward network\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, \n",
        "              xs: Tensor,  # [bsz, target_seq_len, emb_dim]\n",
        "              encoder_output: Tensor,  # [bsz, src_seq_len, emb_dim]\n",
        "              src_mask: Tensor, # (B,1,1,S)\n",
        "              trg_mask: Tensor # (B,1,S,S)\n",
        "              ) -> Tensor:\n",
        "    ys = self.masked_attn(query=xs, kv=xs, mask=trg_mask) # [bsz, target_seq_len, emb_dim]\n",
        "    xs2 = self.norm1(self.dropout(ys) + xs)\n",
        "    ys2 = self.cross_attn(query=xs2, kv=encoder_output, mask=src_mask)\n",
        "    imm = self.norm2(self.dropout(ys2) + xs2)\n",
        "    out = self.ff(imm)\n",
        "    return self.norm2(self.dropout(out) + imm)\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               emb_dim: int = 512,\n",
        "               hidden_dim: int = 2048,\n",
        "               n_heads = 8, \n",
        "               depth: int = 8):\n",
        "    super().__init__()\n",
        "    decoder_block = TransformerDecoderBlock(emb_dim, hidden_dim, n_heads)\n",
        "    self.layers = nn.ModuleList([copy.deepcopy(decoder_block) for i in range(depth)])\n",
        "\n",
        "  def forward(self,\n",
        "              xs: Tensor,  # [bsz,target_seq_len,emb_dim] (where target_seq_len=1 if inference time)\n",
        "              encoder_output: Tensor, # [bsz,src_seq_len,emb_dim] \n",
        "              src_mask: Tensor, # (B,1,1,S)\n",
        "              trg_mask: Tensor, # (B,1,S,S)\n",
        "              ) -> Tensor:\n",
        "    out = xs  \n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, encoder_output, src_mask=src_mask, trg_mask=trg_mask)\n",
        "\n",
        "    return out \n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size: int, emb_dim: int):\n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "  \n",
        "  def forward(self, xs: Tensor):\n",
        "    ys = self.embedding(xs)\n",
        "    return ys * math.sqrt(self.emb_dim)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, \n",
        "               src_vocab_size: int, \n",
        "               trg_vocab_size: int,\n",
        "               emb_dim: int = 512,\n",
        "               hidden_dim: int = 2048,\n",
        "               depth: int = 8,\n",
        "               n_heads: int = 8,\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.src_embedding = Embedding(src_vocab_size, emb_dim)\n",
        "    self.src_pos_encoding = PositionalEncoding(emb_dim)\n",
        "\n",
        "    self.trg_embedding = Embedding(trg_vocab_size, emb_dim)\n",
        "    self.trg_pos_encoding = PositionalEncoding(emb_dim)\n",
        "\n",
        "    self.encoder = TransformerEncoder(emb_dim, hidden_dim, n_heads, depth)\n",
        "    self.decoder = TransformerDecoder(emb_dim, hidden_dim, n_heads, depth)\n",
        "    self.project = nn.Linear(emb_dim, trg_vocab_size)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for name, param in self.named_parameters():\n",
        "      if param.dim() > 1:\n",
        "        # print(\"name: \", name)\n",
        "        nn.init.xavier_uniform_(param)\n",
        "  \n",
        "  def forward(self, \n",
        "              src: Tensor, # [bsz, src_seq_len]\n",
        "              src_padding_mask: Tensor, # (B,1,1,S) -> it can't be (B,1,S,S) because we need this in decoder as well, and no.of rows will be same as target seq length\n",
        "              trg: Tensor, # [bsz, target_seq_len]\n",
        "              trg_mask: Tensor, # (B,1,S,S)\n",
        "              ) -> Tensor:\n",
        "    encoder_output = self.encode(src, src_padding_mask)\n",
        "    return self.decode(trg, encoder_output, src_mask=src_padding_mask, trg_mask=trg_mask) # [bsz, target_seq_len, vocab_size]\n",
        "  \n",
        "  def encode(self, src:Tensor, mask: Tensor) -> Tensor:\n",
        "    src_embeds_batch = self.src_embedding(src) # [bsz, src_seq_len, emb_dim]\n",
        "    src_embeds_batch = self.src_pos_encoding(src_embeds_batch)\n",
        "    out = self.encoder(src_embeds_batch, mask)\n",
        "    return out\n",
        "\n",
        "  def decode(self, trg: Tensor, encoder_output: Tensor, src_mask: Tensor, trg_mask: Tensor) -> Tensor:\n",
        "    trg_embeds_batch = self.trg_embedding(trg)\n",
        "    # print(\"target tensor dim: \", trg.shape)\n",
        "    trg_embeds_batch = self.trg_pos_encoding(trg_embeds_batch)\n",
        "    out = self.decoder(trg_embeds_batch, encoder_output, src_mask=src_mask, trg_mask=trg_mask)\n",
        "    # print(\"actual tensor dim: \", out.shape)\n",
        "    projected_out = self.project(out) # [bsz, trg_seq_len, vocab_size]\n",
        "    # Reshape to make it nice for CrossEntropyLoss :)\n",
        "    logits = projected_out.reshape(-1, projected_out.shape[2])\n",
        "    return logits\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, emb_dim: int, max_seq_len: int = 5000):\n",
        "    super().__init__()\n",
        "    assert emb_dim % 2 == 0, \"Embedding dimension must be divisble by 2\"\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    \n",
        "    pos = torch.arange(max_seq_len)[:, None] # [seq_len, 1]\n",
        "    evens = 10000. ** (-torch.arange(0,emb_dim,step=2) / emb_dim)\n",
        "    evens = evens[None, :] # [1, ceil(emb_dim/2)]\n",
        "    evens = pos * evens # [seq_len, ceil(emb_dim/2)]\n",
        "    pe = rearrange([evens.sin(), evens.clone().cos()], 't h w -> h (w t)') # interleave even and odd parts\n",
        "      \n",
        "    self.register_buffer('pe', pe) # [max_seq_len, emb_dim]\n",
        "  \n",
        "  def forward(self, \n",
        "              src: Tensor # [bsz, seq_len, emb_dim]\n",
        "              ) -> Tensor:\n",
        "    assert src.shape[-1] == self.pe.shape[1], f\"Expected embedding dimension of {self.pe[1]} but got {src.shape[-1]} instead.\"\n",
        "    out = src + self.pe[None,:src.size(1),:]\n",
        "    return self.dropout(out) # See Page 7 of original paper, under section \"Regularization\"\n",
        "\n",
        "  def check_and_plot_pos_encoding(self): \n",
        "    # modified from tensorflow example (https://www.tensorflow.org/text/tutorials/transformer#positional_encoding)\n",
        "\n",
        "    # Check the shape.\n",
        "    print(self.pe.numpy().shape)\n",
        "\n",
        "    # Plot the dimensions.\n",
        "    plt.pcolormesh(self.pe.numpy(), cmap='RdBu')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel('Depth')\n",
        "    plt.ylabel('Position')\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KPcCO97g3ETK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PositionalEncoding(512, 2048).check_and_plot_pos_encoding()"
      ],
      "metadata": {
        "id": "fuuPVzBdFkF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmingLRScheduler(optim.lr_scheduler._LRScheduler):\n",
        "  def __init__(self, \n",
        "               optimizer: optim.Optimizer, \n",
        "               warming_steps: int, \n",
        "               emb_dim: int,\n",
        "               last_epoch:int = -1,\n",
        "               verbose: bool = False):\n",
        "    self.optimizer = optimizer\n",
        "    self.warming_steps = warming_steps\n",
        "    self.emb_dim = emb_dim\n",
        "    self.step_num = 1\n",
        "    super(WarmingLRScheduler, self).__init__(optimizer)\n",
        "  \n",
        "  def get_lr(self) -> List[float]:\n",
        "    steps = self._step_count # pytorch initializes this at 1 \n",
        "    new_lr = self.emb_dim**(-0.5) * min(steps**(-0.5), steps*self.warming_steps**(-1.5))\n",
        "    return [new_lr for _ in self.optimizer.param_groups]\n"
      ],
      "metadata": {
        "id": "Ic8hTY_MGQ5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_words(indices: List[int], vocab: torchtext.vocab.Vocab) -> List[str]:\n",
        "  return [vocab.get_itos()[idx] for idx in indices]"
      ],
      "metadata": {
        "id": "0mT98JRJGpTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_decoder_mask(seq_len: int) -> Tensor:\n",
        "  mask = torch.zeros(seq_len, seq_len).to(device)\n",
        "  mask_indices = torch.arange(seq_len)[None, :] > torch.arange(seq_len)[:, None] \n",
        "  mask[mask_indices] = float('-inf')  \n",
        "  return mask.reshape(1,1,seq_len,seq_len) # (1, 1, S, S)\n",
        "\n",
        "def create_padding_mask(xs: Tensor, # [bsz, seq_len]\n",
        "                        pad_idx: int \n",
        "                        ) -> Tensor:\n",
        "  batch_size, seq_len = xs.shape\n",
        "  mask = torch.zeros(xs.shape).to(device)\n",
        "  mask_indices = xs == pad_idx\n",
        "  mask[mask_indices] = float('-inf')\n",
        "  return mask.reshape(batch_size,1,1,seq_len) # (B, 1, 1, S)\n",
        "\n",
        "def create_padding_mask_old(xs: Tensor, # [bsz, seq_len]\n",
        "                        pad_idx: int \n",
        "                        ) -> Tensor:\n",
        "  bsz, seq_len = xs.shape\n",
        "  mask = torch.zeros(bsz, seq_len, seq_len) # [bsz, seq_len, seq_len]\n",
        "  mask_indices = xs == pad_idx\n",
        "  mask[mask_indices, :] = float('-inf')\n",
        "  mask = mask.permute(0,2,1)\n",
        "  mask[mask_indices, :] = float('-inf')\n",
        "  return mask.permute(0,2,1)"
      ],
      "metadata": {
        "id": "V-zZNETZ4tzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: although this means that softmax of the last row of the second matrix will mean 1/3 for each cell,\n",
        "# This doesn't matter because we do not include the PAD_IDX in the final calculation of loss using nn.CrossEntropyLoss()\n",
        "# See https://stats.stackexchange.com/questions/598239/how-is-padding-masking-considered-in-the-attention-head-of-a-transformer\n",
        "create_padding_mask_old(torch.Tensor([[0,2,1],[1,2,1]]), PAD_IDX)"
      ],
      "metadata": {
        "id": "MeEbdC3jHI8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_padding_mask(torch.Tensor([[0,2,1],[1,2,1]]), PAD_IDX)"
      ],
      "metadata": {
        "id": "npUlCfcKsl-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most of details can be found in original paper\n",
        "EMB_DIM = 512\n",
        "HIDDEN_DIM = 2048\n",
        "LAYER_DEPTH = 6\n",
        "NUM_HEADS = 8\n",
        "NUM_WARMING_STEPS = 4000\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(de_vocab),\n",
        "    trg_vocab_size=len(en_vocab),\n",
        "    emb_dim=EMB_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    depth=LAYER_DEPTH,\n",
        "    n_heads=NUM_HEADS,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(),betas=(0.9, 0.98), eps=1e-9)\n",
        "lr_scheduler = WarmingLRScheduler(optimizer, NUM_WARMING_STEPS, EMB_DIM)"
      ],
      "metadata": {
        "id": "oOrV7sDltkyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum([p.numel() for p in model.parameters() if p.requires_grad])"
      ],
      "metadata": {
        "id": "f2CRm65EMcp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a hack for now\n",
        "optimizer.param_groups[0]['lr'] = EMB_DIM**(-0.5) * min(1**(-0.5), NUM_WARMING_STEPS**(-1.5))"
      ],
      "metadata": {
        "id": "OAOXrWjDoswx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"transformers-train\",\n",
        "    config={\n",
        "    \"num_heads\": NUM_HEADS,\n",
        "    \"layer_depth\": LAYER_DEPTH,\n",
        "    \"hidden_dim\": HIDDEN_DIM,\n",
        "    \"epochs\": NUM_EPOCHS,\n",
        "    \"num_warning_steps\": NUM_WARMING_STEPS,\n",
        "    \"emb_dim\": EMB_DIM,\n",
        "    \"dropout\": True,\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "yIQjBdxeJlIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, optimizer: optim.Optimizer) -> float:\n",
        "  trg_tokens_processed = 0\n",
        "  running_loss = 0.0\n",
        "  num_sentence_pairs = 0\n",
        "\n",
        "  for idx, (src, target) in enumerate(train_dl):\n",
        "    # Move to GPU\n",
        "    target = target.to(device)\n",
        "    src = src.to(device)\n",
        "\n",
        "    # src is german, target is english\n",
        "    target_input = target[:,:-1] # Shifted left\n",
        "    bsz, trg_seq_len = target_input.shape\n",
        "\n",
        "    target_predict = target[:,1:]\n",
        "    target_predict_flatten = target_predict.reshape(-1) # Shifted right\n",
        "\n",
        "    # Get number of tokens processed so we can track it, ignores <pad> in count \n",
        "    trg_tokens_processed += torch.sum((target_input != PAD_IDX).long())\n",
        "\n",
        "    # each mini batch [bsz, seq_len] => seq_len changes (and bsz too, for last)\n",
        "    src_seq_len = src.shape[1]\n",
        "    target_seq_len = target_input.shape[1]\n",
        "\n",
        "    # Create masks\n",
        "    target_mask = create_padding_mask(target_input, PAD_IDX) + create_decoder_mask(target_seq_len) # (B,1,S,S)\n",
        "    src_mask = create_padding_mask(src, PAD_IDX) # (B,1,1,S)\n",
        "\n",
        "    out = model(src, src_mask, target_input, target_mask) # [bsz, target_seq_len, target_vocab_size]\n",
        "\n",
        "    probs = F.softmax(out, dim=-1)\n",
        "    preds = probs.argmax(dim=-1).reshape(bsz, trg_seq_len)\n",
        "\n",
        "    training_loss = criterion(out, target_predict_flatten)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    training_loss.backward()\n",
        "\n",
        "    running_loss += training_loss.item()\n",
        "    num_sentence_pairs += src.size(0) # batch size, should be 128 most of the time\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # print(f\"steps_taken: {steps_taken} | scheduler step count: {lr_scheduler._step_count}\")\n",
        "\n",
        "    # Diagnostic logging\n",
        "    if idx % 50 == 0:\n",
        "      print(\"German: \", idx_to_words(src[0].tolist(), de_vocab))\n",
        "      print(\"English (actual): \", idx_to_words(target_predict[0].tolist(), en_vocab))\n",
        "      print(\"English (predicted): \", idx_to_words(preds[0].tolist(), en_vocab))\n",
        "      \n",
        "      print(f\"epoch={epoch:3d} | batch={idx+1:3d}/{len(train_dl)} \" \n",
        "            f\"| avg. tokens/batch={trg_tokens_processed/50}\" \n",
        "            f\"| avg. running loss={running_loss/(idx+1): 5.2f}]\")\n",
        "    \n",
        "      trg_tokens_processed = 0\n",
        "\n",
        "      wandb.log({'train_loss': training_loss.item()})\n",
        "\n",
        "  \n",
        "  return running_loss"
      ],
      "metadata": {
        "id": "QIWHRk3QDC0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "  epoch_start = time.time()\n",
        "\n",
        "  # Start of training \n",
        "  train_loss = train(model, optimizer)\n",
        "  print('-' * 89)\n",
        "  print(f'| end of epoch {epoch:3d} | time {time.time()-epoch_start:5.2f}s | train_loss {train_loss:8.3f} | valid_loss: TODO |')\n",
        "  print('-' * 89)"
      ],
      "metadata": {
        "id": "1RN4BU4B_VWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "RXu4rVkOnKcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, max_seq_len: int):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    val_src = next(iter(val_dl))[0][0].unsqueeze() # [1,src_seq_len]\n",
        "    memory = model.encode(val_src)\n",
        "    xs = torch.zeros(1,1).fill_(BOS_IDX).type_as(val_src.data) # [1,1]\n",
        "\n",
        "    for i in range(max_seq_len):\n",
        "      probs = model.decode(xs, memory, mask=None) # [1, seq_len,target_vocab_size]\n",
        "      preds = probs.argmax(dim=-1) # [1,seq_len]\n",
        "      assert preds.shape[0] == 1 and preds.shape[1] == 1\n",
        "      xs = preds\n"
      ],
      "metadata": {
        "id": "nqV7qlFkDMQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model greedily\n",
        "evaluate(model)"
      ],
      "metadata": {
        "id": "5EOIY0m9DSlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BxTJ5cZvARV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}